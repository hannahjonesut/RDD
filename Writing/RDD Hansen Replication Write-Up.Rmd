---
title: "RDD Hansen Replication"
author: "Hannah Jones"
date: "3/5/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, echo= FALSE, include=FALSE, warning=TRUE}

library(tidyverse)
library(haven)
library(stargazer)
library(estimatr)
library(rdd)
library(rdrobust)
library(rddensity)
library(ggplot2)
library(dplyr)
library(kableExtra)
library(ggpubr)

dwi <- read.csv('https://raw.githubusercontent.com/hannahjonesut/causal-inference-class/master/Data/hansen_dwi.csv')

knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

## GitHub Repo
In this write up I have only included figures and graphs.  To find the full .Rmd file please go to:(https://github.com/hannahjonesut/RDD/blob/main/Writing/RDD%20Hansen%20Replication%20Write-Up.Rmd).  For the full .R code file go to: (https://github.com/hannahjonesut/RDD/blob/main/R%20Scripts/RDD_Hansen.R).

## Summary

The paper is studying the effect of severity of punishment on recidivism.  In this case, the author is looking at drunk driving and the impact of punishments that increase in severity with the increase in blood-alcohol level.  The paper aims to provide some basis for analyzing whether harsher BAC cutoffs and punishments will dis-incentivize the commitment of the offense (drunk driving).  The author is using data from ~500k DUI stops from the state of Washington from 1995 to 2011.  Notably, Washington instituted a 0.08 cut-off for DUI and 0.15 for aggravated DUI in 1999.  So, the author uses the 1999-2007 to look at the impact of having a BAC of 0.08 or 0.15 on recidivism.  The author is looking at discrete BAC thresholds of 0.08 and 0.15 to determine the impact of punishment on recidivism, as a BAC of 0.08 will result in punishment, and a BAD of 0.15 will result in harsher punishment.  The author finds that punishment serves as a good deterrent for drunk driving.  This suggests that lowering the BAC cutoff from 0.08 to 0.05 will be effective in deterring drunk driving behavior and recidivism.

## Replication

3. I created the variable bac_high as a dummy variable to denote BAC over 0.08.
```{r}

#create a dummy variable for BAC >= 0.08
dwi <- dwi %>%
  mutate( 
    bac_high  = if_else(bac1 >= 0.08, 1, 0))
```

4. The graph below shows the frequency of observations at each BAC level, with the red line showing the cutoff of 0.08.  The distribution is an even bell curve, with no bunching just below the cutoff.  This suggests, as discussed in the paper, that there is no maniulation of BAC at the cutoff.  This affirms that people being pulled over are not able to manipulate their BAC to avoid punishment at the cutoff.
```{r}
#Number 4-- sort on running variable to see if any manipulation
bac1_recid = dwi %>%
  group_by(bac1)%>%
  summarize(sum_recid = sum(recidivism))

ggplot(data = bac1_recid) +
  geom_col(aes(x = bac1, y = sum_recid)) +
  labs(
    title = "Frequency of Recidivism vs Blood Alcohol Content Histogram",
    caption = "Based on administrative records from the Washington State Impaired Driver Testing Program, 1999â€“2007.Vertical line at BAC = 0.08.",
    x="Blood Alcohol Content",
    y="Frequency") +
  geom_vline(xintercept = .08, colour = "red", linetype = 1) 
```

5. Controlling for equation 1 and testing the cutoff at 0.08 yields the results in the table below.  As shown below by the standard errors and p-values, I fail to reject the null hypothesis that these characteristics are unrelated to the BAC cutoff of 0.08. This conclusion is supported graphically in the next problem. 

```{r}
#No 5 use RDD

cov_male <- RDestimate(male~bac1 | aged+acc+white+bac_high+bac1*bac_high, 
                       data = dwi,cutpoint = 0.08, bw = 0.05, kernel = "rectangular")

cov_age<- RDestimate(aged~bac1 | male+acc+white+bac_high+bac1*bac_high, 
                     data = dwi,cutpoint = 0.08, bw = 0.05, kernel = "rectangular")

cov_acc<- RDestimate(acc~bac1 | aged+male+white+bac_high+bac1*bac_high, 
                     data = dwi,cutpoint = 0.08, bw = 0.05, kernel = "rectangular")

cov_white<- RDestimate(white~bac1 | aged+acc+male+bac_high+bac1*bac_high, 
                       data = dwi,cutpoint = 0.08, bw = 0.05, kernel = "rectangular")

cov_bal<-data.frame(
  Estimates = c("Coefficient", "SE", "Z", "P-Value", "Confidence Interval"),
  malecov = c(cov_male$est[1], cov_male$se[1], cov_male$z[1], cov_male$p[1], cov_male$ci[1]),
  agecov = c(cov_age$est[1], cov_age$se[1], cov_age$z[1], cov_age$p[1], cov_age$ci[1]),
  acccov = c(cov_acc$est[1], cov_acc$se[1], cov_acc$z[1], cov_acc$p[1], cov_acc$ci[1]),
  whitecov = c(cov_white$est[1], cov_white$se[1], cov_white$z[1], cov_white$p[1], cov_white$ci[1])
)
  
cov_bal %>%
  kable(
    col.names = c("Characteristic", "Male", "Age", "Accident", "White"),
    digits = 3,
    caption = "Panel A Covariate Balance"
  )%>%
  kable_classic(full_width = F, html_font = "Cambria")
  



```
6. Below are the graphs for covariate balance.  The BAC = 0.08 cutoff is represented by the blue vertical line, and the confidence intervals are shown by the blue dotted lines.  These graphs agree with Hansen's conclusions that there are no statistically significant jumps in likelihood of recidivism at the 0.08 cutoff.  This lends additional credibility to the assertion that neither the offender nor the police are capable of manipulating data at the cutoff.  This is an important base to prove unbiasedness moving forward with the design.

```{r, warning= FALSE}
#Recreate figure 2
dwi_cov = dwi %>%
  group_by(bac1)%>%
  summarize(n=n(), male_prop = sum(male)/n, white_prop = sum(white)/n, acc_prop = sum(acc)/n, aged_prop = sum(aged)/n)

#MODEL setup
X <- dwi$bac1 - .08 # center the data
Y_male <- dwi$male

X2 <- (dwi$bac1 - .08)^2 

Xl <- (X < 0) * X # X if X < 0, o.w. 0
Xl2 <- Xl^2
Xr <- (X >= 0) * X # X if X >= 0, o.w. 0
Xr2 <- Xr^2
Tr <- as.integer(X >= 0) # dummy variable


weights <- rdd::kernelwts(X, center = 0, bw = .05, kernel = "rectangular")

#male
model_male <- lm(Y_male ~ Tr + Xl + Xr, weights = weights)

male_df <- data.frame(male_pred = predict(model_male, dwi), bac1 = dwi$bac1)
male_CI <- data.frame(male_CI = predict(model_male, dwi, interval = "confidence"),  bac1 = dwi$bac1)

male_graph <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= male_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = male_df, aes(y = male_pred))+
  geom_line(data= male_CI, aes(y = male_CI.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= male_CI, aes(y = male_CI.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(0.6,0.9)

quad_male <- lm(Y_male ~ Tr + Xl + Xl2 + Xr + Xr2, weights = weights)

male_df_q <- data.frame(male_pred_q = predict(quad_male, dwi), bac1 = dwi$bac1)
male_CI_q <- data.frame(male_CI_q = predict(quad_male, dwi, interval = "confidence"),  bac1 = dwi$bac1)

male_graph_q <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= male_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = male_df_q, aes(y = male_pred_q))+
  geom_line(data= male_CI_q, aes(y = male_CI_q.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= male_CI_q, aes(y = male_CI_q.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(0.6,0.9)

#white
Y_white <- dwi$white

model_white <- lm(Y_white ~ Tr + Xl + Xr, weights = weights)
white_CI <- data.frame(white_CI = predict(model_white, dwi, interval = "confidence"),  bac1 = dwi$bac1)

white_df <- data.frame(white_pred = predict(model_white, dwi), bac1 = dwi$bac1)
white_graph <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= white_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = white_df, aes(y = white_pred))+
  geom_line(data= white_CI, aes(y = white_CI.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= white_CI, aes(y = white_CI.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(0.75,0.9)


#white quadratic

quad_white <- lm(Y_white ~ Tr + Xl + Xl2 + Xr + Xr2, weights = weights)

white_df_q <- data.frame(white_pred_q = predict(quad_white, dwi), bac1 = dwi$bac1)
white_CI_q <- data.frame(white_CI_q = predict(quad_white, dwi, interval = "confidence"),  bac1 = dwi$bac1)

white_graph_q <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= white_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = white_df_q, aes(y =white_pred_q))+
  geom_line(data= white_CI_q, aes(y = white_CI_q.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= white_CI_q, aes(y = white_CI_q.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(0.75,0.9)



#acc
Y_acc <- dwi$acc

model_acc <- lm(Y_acc ~ Tr + Xl + Xr, weights = weights)
acc_CI <- data.frame(acc_CI = predict(model_acc, dwi, interval = "confidence"),  bac1 = dwi$bac1)

acc_df <- data.frame(acc_pred = predict(model_acc, dwi), bac1 = dwi$bac1)
acc_graph <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= acc_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = acc_df, aes(y = acc_pred))+
  geom_line(data= acc_CI, aes(y = acc_CI.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= acc_CI, aes(y = acc_CI.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(0, 0.3)


#acc quadratic

quad_acc <- lm(Y_acc ~ Tr + Xl + Xl2 + Xr + Xr2, weights = weights)

acc_df_q <- data.frame(acc_pred_q = predict(quad_acc, dwi), bac1 = dwi$bac1)
acc_CI_q <- data.frame(acc_CI_q = predict(quad_acc, dwi, interval = "confidence"),  bac1 = dwi$bac1)

acc_graph_q <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= acc_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = acc_df_q, aes(y =acc_pred_q))+
  geom_line(data= acc_CI_q, aes(y = acc_CI_q.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= acc_CI_q, aes(y = acc_CI_q.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(0, 0.3)


#age
Y_aged <- dwi$aged

model_aged <- lm(Y_aged ~ Tr + Xl + Xr, weights = weights)
aged_CI <- data.frame(aged_CI = predict(model_aged, dwi, interval = "confidence"),  bac1 = dwi$bac1)

aged_df <- data.frame(aged_pred = predict(model_aged, dwi), bac1 = dwi$bac1)
aged_graph <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= aged_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = aged_df, aes(y = aged_pred))+
  geom_line(data= aged_CI, aes(y = aged_CI.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= aged_CI, aes(y = aged_CI.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(32,42)


#aged quadratic

quad_aged <- lm(Y_aged ~ Tr + Xl + Xl2 + Xr + Xr2, weights = weights)

aged_df_q <- data.frame(aged_pred_q = predict(quad_aged, dwi), bac1 = dwi$bac1)
aged_CI_q <- data.frame(aged_CI_q = predict(quad_aged, dwi, interval = "confidence"),  bac1 = dwi$bac1)

aged_graph_q <- ggplot(dwi_cov, aes(x = bac1))+
  geom_point(aes(y= aged_prop), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = aged_df_q, aes(y =aged_pred_q))+
  geom_line(data= aged_CI_q, aes(y = aged_CI_q.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= aged_CI_q, aes(y = aged_CI_q.lwr), color = "steelblue", linetype = 3 )+
  xlim(0, 0.2)+
  ylim(32,42)


#combine graphs
ggarrange(male_graph, male_graph_q, 
          labels = c("Male Linear", "Male Quadratic"),
          ncol = 2, nrow = 1)

ggarrange(white_graph, white_graph_q,
          labels = c("White Linear", "White Quadratic"),
          ncol = 2, nrow = 1)

ggarrange(acc_graph, acc_graph_q, 
          labels = c("Acc Linear", "Acc Quadratic"),
          ncol = 2, nrow = 1)

ggarrange( aged_graph, aged_graph_q,
          labels = c("Age Linear", "Age Quadratic"),
          ncol = 2, nrow = 1)


```


7. The chart below shows the effect of the BAC threshold on recidivism for two different bandwidths.  These results are found using models that control for the demographic characteristics discussed previously. These results suggest statistically significant effects at the cutoff for the linear and the interacted models.  For both the linear and interacted models with both bandwidths, having a BAC over the 0.08 cutoff reduces recidivism by around 2%. These estimates are in line with those in the paper.  

```{r}
#No 7
dwiA= dwi %>% 
  filter(bac1>0.03 & bac1 < 0.13)%>%
  mutate(bac2 = bac1*bac1, bac7 = bac1-0.08, dui = ifelse(bac7 >= 0,1,0))

dwiB= dwi %>% 
  filter(bac1>0.055 & bac1 < 0.105)%>%
  mutate(bac2 = bac1*bac1, bac7 = bac1-0.08, dui = ifelse(bac7 >= 0,1,0))

weightsA <- kernelwts(dwiA$bac7, center = 0, bw = .05, kernel = "rectangular")
weightsB <- kernelwts(dwiB$bac7, center = 0, bw = .05, kernel = "rectangular")

linearA <- lm_robust(recidivism ~ bac7 + dui + aged + white + male + acc,
              data = dwiA,
              weights = weightsA)
linearB <- lm_robust(recidivism ~ bac7 + dui + aged + white + male + acc,
              data = dwiB,
              weights = weightsB)


linearcutA <- lm_robust(recidivism ~ bac7*dui + aged + white + male + acc,
            data = dwiA,
            weights = weightsA)
linearcutB <- lm_robust(recidivism ~ bac7*dui + aged + white + male + acc,
              data = dwiB,
              weights = weightsB)


linquadcutA <- lm_robust(recidivism ~ dui*(bac7+bac2) + aged + white + male + acc,
                 data = dwiA,
                 weights = weightsA)
linquadcutB <- lm_robust(recidivism ~ dui*(bac7+bac2) + aged + white + male + acc,
                 data = dwiB,
                 weights = weightsB)


tab3_models<-data.frame(
  Panels = c("Bandwidth 0.03 to 0.13", "Std. Error", "Bandwidth 0.055 to 0.105", "Std. Error"),
  linear = c(linearA$coefficients[3], linearA$std.error[3], linearB$coefficients[3],linearA$std.error[3]),
  linearint = c(linearcutA$coefficients[3] , linearcutA$std.error[3], linearcutB$coefficients[3], linearcutB$std.error[3]),
  quadint = c(linquadcutA$coefficients[2], linquadcutA$std.error[2], linquadcutB$coefficients[2], linquadcutB$std.error[2])
  
)

tab3_models %>%
  kable(
    col.names = c("Panels", "Linear", "Linear with Interaction", "Quadratic with Interaction"),
    digits = 3,
    caption = "Treatment Effect Models"
  )%>%
  kable_classic(full_width = F, html_font = "Cambria")
```


8. The graphs below were created using only observations below BAC = 0.15.  These graphs agree with the chart above, showing a statistically significant treatment effect at the BAC cutoff of 0.08 for the linear model, and a non-significant effect for the quadratic model.

```{r warning=FALSE}

dwi8 = dwi %>% 
  filter(bac1 <= 0.15)%>%
  group_by(bac1)%>%
  summarize(n=n(), recid_prob = sum(recidivism)/n)


X <- dwi8$bac1 - .08 # center the data
Y <- dwi8$recid_prob
X2 <- X^2 

Xl <- (X < 0) * X # X if X < 0, o.w. 0
Xl2 <- Xl^2

Xr <- (X >= 0) * X # X if X >= 0, o.w. 0
Xr2 <- Xr^2

Tr <- as.integer(X >= 0) # dummy variable


weights <- rdd::kernelwts(X, center = 0, bw = .05, kernel = "rectangular")

model8 <- lm_robust(Y ~ Tr + Xl + Xr, data=dwi8, weights = weights)
quad8 <- lm_robust(Y ~ Tr +Xl2 + Xl +Xr2 + Xr, data=dwi8, weights = weights)

df <- data.frame(pred = predict(model8, dwi8), bac1 = dwi8$bac1)
CI <- data.frame(CI = predict(model8, dwi8, interval = "confidence"),  bac1 = dwi8$bac1)

df_q<- data.frame(pred_q = predict(quad8, dwi8), bac1 = dwi8$bac1)
CI_q <- data.frame(CI_q = predict(quad8, dwi8, interval = "confidence"),  bac1 = dwi8$bac1)

graph <- ggplot(dwi8, aes(x = bac1))+
  geom_point(aes(y= Y), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = df, aes(y = pred))+
  geom_line(data= CI, aes(y = CI.fit.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= CI, aes(y = CI.fit.lwr), color = "steelblue", linetype = 3 )


graph_q <- ggplot(dwi8, aes(x = bac1))+
  geom_point(aes(y= Y), alpha = 0.25)+
  geom_vline(xintercept = 0.08, colour = "blue", linetype = 1)+
  geom_line(data = df_q, aes(y = pred_q))+
  geom_line(data= CI_q, aes(y = CI_q.fit.upr), color = "steelblue", linetype = 3 )+
  geom_line(data= CI_q, aes(y = CI_q.fit.lwr), color = "steelblue", linetype = 3 )+
  xlim(0,0.15)+
  ylim(0.04,0.15)


#combine graphs
ggarrange(graph, graph_q,
          labels = c("Linear Model", "Quadratic Model"),
          ncol = 1, nrow = 2)

```


9. This exercise succeeded in teaching me concepts of Causal Inference, as well as providing a crash course in using R.  In this exercise, I first found that the covariates were balanced, which means that the covariates do not jump in expected value at the BAC cutoff.  This is an important finding to suggest that both cops and offenders are not able to manipulate the outcome of a BAC test at the cutoff based on various characteristics.  Next, I learned how RDD is just an extension of linear models.  So, I used linear and quadratic models to look at the treatment effect of the BAC cutoff on the probability of recidivism.  I was testing the null hypothesis that the BAC cutoff policy has no effect on whether or not an offender commits a repeat DUI offense.  I found that there is a statistically significant effect of the "treatment" (BAC cutoff) when using the linear model, though a quadratic model yields a non-significant effect.  Hansen concludes that the BAC cutoff successfully decreases the likelihood of a repeat offense.  I agree with this conclusion due to the test of covariate balance and the subsequent test of treatment effect at the cutoff.  However, I am less confident in the finding due to the elimination of a significant treatment effect when using a quadratic model.  


